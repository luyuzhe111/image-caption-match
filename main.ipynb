{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luyuzhe111/image-caption-match/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7VL_cb9YIFm",
        "outputId": "c3a85382-0813-4de0-bb0d-0eee4caa3785"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64P31INAXzq2"
      },
      "source": [
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GVxRe8GX_BT"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/kaggle/wikipedia')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF3OfFHPYDzl"
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OENiT24ZPD4"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDwS9F2GZcqB"
      },
      "source": [
        "! pip install -q albumentations==0.4.6"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWSvtDspHO2n",
        "outputId": "d82f4384-64c4-4507-87d2-94e1642428db"
      },
      "source": [
        "! pip install cairosvg"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cairosvg in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from cairosvg) (0.7.1)\n",
            "Requirement already satisfied: cssselect2 in /usr/local/lib/python3.7/dist-packages (from cairosvg) (0.4.1)\n",
            "Requirement already satisfied: cairocffi in /usr/local/lib/python3.7/dist-packages (from cairosvg) (1.3.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.7/dist-packages (from cairosvg) (1.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from cairosvg) (7.1.2)\n",
            "Requirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi->cairosvg) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.21)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from cssselect2->cairosvg) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1aj6-Y_Zctq",
        "outputId": "7b419853-5f44-43b9-c0de-34cbc04b83ae"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS0iEjoMEbqd",
        "outputId": "383d05ce-5cb6-465d-ce79-7c537a840e4b"
      },
      "source": [
        "!pip install pytorch-metric-learning"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-metric-learning in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.10.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (1.4.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch-metric-learning) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOLZfZ-zZSkA"
      },
      "source": [
        "import glob\n",
        "import gc\n",
        "gc.enable()\n",
        "import multiprocessing\n",
        "import cv2\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "\n",
        "import base64\n",
        "import pickle\n",
        "\n",
        "# fold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# For downloading images\n",
        "from io import BytesIO\n",
        "\n",
        "# For data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import asarray\n",
        "\n",
        "# Pytorch Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda import amp\n",
        "import torchvision\n",
        "\n",
        "\n",
        "# Utils\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# Sklearn Imports\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "\n",
        "# For Image Models\n",
        "# import timm\n",
        "\n",
        "# For Transformer Models\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypo2ZP7q-o_A"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnZOZlxxYPIB"
      },
      "source": [
        "# load data from input files\n",
        "df_train = pd.read_feather('./train-subsample.feather')\n",
        "df_valid = pd.read_feather('./valid-subsample.feather')\n",
        "df_test = pd.read_feather('./test-subsample.feather')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "W447hUeTYYnm",
        "outputId": "e8de30e8-1576-4f26-fcda-fce98c5be906"
      },
      "source": [
        "df_train.head(3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>language</th>\n",
              "      <th>image_url</th>\n",
              "      <th>caption_title_and_reference_description</th>\n",
              "      <th>page_title</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sk</td>\n",
              "      <td>http://upload.wikimedia.org/wikipedia/commons/...</td>\n",
              "      <td>Edubuntu &lt;/s&gt; Edubuntu 7.04 - Gaim a napaľovanie</td>\n",
              "      <td>Edubuntu</td>\n",
              "      <td>./images/images0/sk/13737.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>en</td>\n",
              "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
              "      <td>Lensfield Road &lt;/s&gt;</td>\n",
              "      <td>Lensfield Road</td>\n",
              "      <td>./images/images4/en/95883.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>en</td>\n",
              "      <td>http://upload.wikimedia.org/wikipedia/commons/...</td>\n",
              "      <td>Argao &lt;/s&gt; Old cannons of Argao</td>\n",
              "      <td>Argao</td>\n",
              "      <td>./images/images3/en/66896.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  language  ...                           path\n",
              "0       sk  ...  ./images/images0/sk/13737.jpg\n",
              "1       en  ...  ./images/images4/en/95883.jpg\n",
              "2       en  ...  ./images/images3/en/66896.jpg\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntBSSaxJHzBy",
        "outputId": "6f58d800-70eb-45e1-9938-532de868fa02"
      },
      "source": [
        "print(f'size of the train/val/test dataset: {len(df_train)}, {len(df_valid)}, {len(df_test)}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of the train/val/test dataset: 79342, 9918, 9918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BR48HHObVOg"
      },
      "source": [
        "# Set up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME7h3Zj5SrjU"
      },
      "source": [
        "## config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdhVHJ8pbX9u"
      },
      "source": [
        "def optimal_num_of_loader_workers():\n",
        "    num_cpus = multiprocessing.cpu_count()\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
        "    return optimal_value"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3VmBHXtZCd6"
      },
      "source": [
        "CONFIG = {\n",
        "    \"seed\": 2021,\n",
        "    \"epochs\": 5,\n",
        "    \n",
        "    \"img_size\": 224,\n",
        "    \"embedding_size\": 512,\n",
        "    \"text_model_name\": \"xlm-roberta-base\",\n",
        "    \"pool\": 'gem-2',\n",
        "    \"freeze_image_model\": False,\n",
        "    \"freeze_text_model\": False,\n",
        "    \"train_batch_size\": 32,\n",
        "    \"valid_batch_size\": 32,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"scheduler\": 'CosineAnnealingLR',\n",
        "    \"min_lr\": 1e-6,\n",
        "    'num_workers':4,\n",
        "    \n",
        "    \"T_max\": 500,\n",
        "    \"weight_decay\": 1e-6,\n",
        "    \"max_length\": 32,\n",
        "    \n",
        "    \"n_accumulate\": 1,\n",
        "}\n",
        "\n",
        "CONFIG[\"experiment\"] = f\"dim-{CONFIG['embedding_size']}_pool-{CONFIG['pool']}\"\n",
        "CONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['text_model_name'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLVnTEbcbatZ"
      },
      "source": [
        "def set_seed(seed=42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(CONFIG['seed'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gAKHa6CfmJ1"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAxt_af32X56"
      },
      "source": [
        "def load_img(dir):\n",
        "  image = Image.open(dir)\n",
        "  return image"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRmiGA6Gb1cx"
      },
      "source": [
        "class WikipediaDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length, transforms=None):\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        self.max_len = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = load_img(self.data.at[index, \"path\"])\n",
        "        if self.transforms:\n",
        "          img = self.transforms(img)\n",
        "        \n",
        "        caption = self.data.at[index, \"caption_title_and_reference_description\"]\n",
        "        caption = caption.replace(\"[SEP]\", \"</s>\") # sep token for xlm-roberta\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            caption,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length'\n",
        "        )\n",
        "        \n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "       \n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'image': img,\n",
        "        }"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVblG88FGItB"
      },
      "source": [
        "# Data Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcdP2TN4GFmE"
      },
      "source": [
        "data_transforms = {\n",
        "    \"train\": transforms.Compose([   \n",
        "        transforms.RandomHorizontalFlip(),                                      \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ]),\n",
        "    \n",
        "    \"valid\": transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ])\n",
        "}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQn36Z0scpcm"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj-EBq7-b1jl"
      },
      "source": [
        "train_dataset = WikipediaDataset(df_train, CONFIG[\"tokenizer\"], CONFIG[\"max_length\"], transforms=data_transforms[\"train\"])\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], num_workers=CONFIG['num_workers'], shuffle=True, pin_memory=True)\n",
        "\n",
        "valid_dataset = WikipediaDataset(df_valid, CONFIG[\"tokenizer\"], CONFIG[\"max_length\"], transforms=data_transforms[\"valid\"])\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=CONFIG['num_workers'], shuffle=False, pin_memory=True)\n",
        "\n",
        "test_dataset = WikipediaDataset(df_test, CONFIG[\"tokenizer\"], CONFIG[\"max_length\"], transforms=data_transforms[\"valid\"])\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=CONFIG['num_workers'], shuffle=False, pin_memory=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5hI8kQaZhgF"
      },
      "source": [
        "# Image model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRjGUB-Sb1mP"
      },
      "source": [
        "from torchvision.models import resnet50"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbvQNzKSvXAK"
      },
      "source": [
        "class ImageFeatureExtractor(nn.Module):\n",
        "    def __init__(self, projector=[2048, 512, 64, 8], pool='gem-1'):\n",
        "        super(ImageFeatureExtractor, self).__init__()\n",
        "        model = resnet50(pretrained=True)\n",
        "        modules = nn.ModuleList(model.children())[:-2]\n",
        "        self.features = nn.Sequential(*modules)\n",
        "\n",
        "        arch=[]\n",
        "        for i in range(1,len(projector)):\n",
        "            arch.append(nn.Linear(projector[i-1], projector[i]))\n",
        "            if i!=len(projector)-1:\n",
        "                arch.append(nn.ReLU())\n",
        "        \n",
        "        self.projector=nn.Sequential(*arch)\n",
        "        self.pool = pool\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = self.features(inputs)\n",
        "        output = torch.flatten(output, start_dim=-2)\n",
        "        if self.pool == 'gem-1':\n",
        "          output = output.mean(-1)\n",
        "        elif self.pool == 'gem-2':\n",
        "          output = torch.pow((output ** 2).mean(-1), 1/2)\n",
        "        elif self.pool == 'gem-inf':\n",
        "          output = output.max(-1)\n",
        "        output = self.projector(output)\n",
        "        return output"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XLvuWMub1oi"
      },
      "source": [
        "projector = [2048, CONFIG['embedding_size']]\n",
        "image_model = ImageFeatureExtractor(projector=projector, pool=CONFIG['pool']).cuda()\n",
        "image_model.train()\n",
        "if CONFIG['freeze_image_model']:\n",
        "  print('freeze image encoder features')\n",
        "  for name, p in image_model.named_parameters():\n",
        "    if 'features' in name:\n",
        "      p.requires_grad = False"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD20LuByZ2-B"
      },
      "source": [
        "# Text model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5PRdagGOJvq"
      },
      "source": [
        "class TextExtractorModel(nn.Module):\n",
        "    def __init__(self, text_model, projector=[748, 96, 32]):\n",
        "        super(TextExtractorModel, self).__init__()\n",
        "        self.text_model = AutoModel.from_pretrained(text_model)\n",
        "\n",
        "        arch=[]\n",
        "        for i in range(1,len(projector)):\n",
        "            arch.append(nn.Linear(projector[i-1], projector[i]))\n",
        "            if i!=len(projector)-1:\n",
        "                arch.append(nn.ReLU())\n",
        "        \n",
        "        self.projector=nn.Sequential(*arch)\n",
        "        self.init_weights(self.projector)\n",
        "        \n",
        "    def init_weights(self, m):\n",
        "        if type(m) == torch.nn.Linear:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0)\n",
        "            \n",
        "    def forward(self, ids, mask):\n",
        "        out = self.text_model(input_ids=ids, attention_mask=mask, output_hidden_states=False)[1]\n",
        "        text_embeddings = self.projector(out)\n",
        "        return text_embeddings"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Wl7BNPPsWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf69ef1b-f2bb-400c-c5b3-614cd5e79e6f"
      },
      "source": [
        "projector=[768, CONFIG['embedding_size']]\n",
        "text_model = TextExtractorModel(CONFIG['text_model_name'], projector=projector).cuda()\n",
        "text_model.train()\n",
        "if CONFIG['freeze_text_model']:\n",
        "  print('freeze text encoder features')\n",
        "  for name, p in text_model.named_parameters():\n",
        "    if 'text_model' in name:\n",
        "      p.requires_grad = False"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTWCE5MgabvB"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtyjQgxS75M"
      },
      "source": [
        "optimizer = optim.Adam([\n",
        "                {'params': image_model.parameters()},\n",
        "                {'params': text_model.parameters()}\n",
        "            ], lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'], betas=(0.9, 0.999), amsgrad=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvVKVRHVadp3"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1mAWIZKTD6R"
      },
      "source": [
        "from pytorch_metric_learning import distances, losses, miners\n",
        "loss_func = losses.TripletMarginLoss(distance=distances.LpDistance())\n",
        "miner = miners.MultiSimilarityMiner()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw4VNzAI2uEQ"
      },
      "source": [
        "# Training & Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFl2QtBbb1qw"
      },
      "source": [
        "def train(epoch, image_model, text_model):\n",
        "  losses = []\n",
        "  image_model.train()\n",
        "  text_model.train()\n",
        "  for index, item in enumerate(tqdm(train_loader)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # transformed image\n",
        "    img = item['image'].cuda()\n",
        "    \n",
        "    # tokens that define the captions\n",
        "    mask = item['mask'].cuda()\n",
        "    ids = item['ids'].cuda()          \n",
        "    \n",
        "    # get encoded images; N x feature_dim (N = batch size)\n",
        "    image_outputs = image_model(img)\n",
        "\n",
        "    # get encoded captions; N x feature_dim\n",
        "    text_outputs = text_model(ids, mask)\n",
        "\n",
        "    embeddings = torch.cat((image_outputs, text_outputs), dim=0) # 2N x feature_dim\n",
        "\n",
        "    batch_size = image_outputs.shape[0]\n",
        "    labels = torch.arange(batch_size)\n",
        "    labels = torch.cat([labels, labels], dim=0)\n",
        "\n",
        "    hard_pairs = miner(embeddings, labels)\n",
        "    loss = loss_func(embeddings, labels, hard_pairs)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "  return np.array(losses).mean()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzHcLTy5yaAp"
      },
      "source": [
        "def validate(epoch, image_model, text_model, model_dir):\n",
        "  losses = []\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    image_model.eval()\n",
        "    text_model.eval()\n",
        "    for index, item in enumerate(tqdm(valid_loader)):\n",
        "      img = item['image'].cuda()\n",
        "      mask = item['mask'].cuda()\n",
        "      ids = item['ids'].cuda()          \n",
        "      \n",
        "      image_outputs = image_model(img)\n",
        "      text_outputs = text_model(ids, mask)\n",
        "\n",
        "      embeddings = torch.cat((image_outputs, text_outputs), dim=0) # 2N x feature_dim\n",
        "      batch_size = image_outputs.shape[0]\n",
        "      labels = torch.arange(batch_size)\n",
        "      labels = torch.cat([labels, labels], dim=0)\n",
        "\n",
        "      hard_pairs = miner(embeddings, labels)\n",
        "      loss = loss_func(embeddings, labels, hard_pairs)\n",
        "\n",
        "      losses.append(loss.item())\n",
        "    \n",
        "    torch.save(image_model.state_dict(), f'{model_dir}/image_model_epoch{epoch}.pt') \n",
        "    torch.save(text_model.state_dict(), f'{model_dir}/text_model_epoch{epoch}.pt') \n",
        "\n",
        "  return np.array(losses).mean()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYY7gL4jybsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57725a21-ab01-4496-dc17-49bb1ab41539"
      },
      "source": [
        "train_losses = []\n",
        "valid_losses = []\n",
        "best_val_loss = 1000\n",
        "best_epoch = 0\n",
        "epochs = 15\n",
        "exp_name = CONFIG['experiment']\n",
        "model_dir = f'./checkpoints/{exp_name}'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = train(epoch, image_model, text_model)\n",
        "  valid_loss = validate(epoch, image_model, text_model, model_dir)\n",
        "\n",
        "  print(train_loss, valid_loss)\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)\n",
        "\n",
        "  if valid_loss < best_val_loss:\n",
        "    print(f'loss has been decreased from {best_val_loss} to {valid_loss}')\n",
        "    best_val_loss = valid_loss\n",
        "    best_epoch = epoch\n",
        "\n",
        "    torch.save(image_model.state_dict(), f'{model_dir}/best_image_model.pt') \n",
        "    torch.save(text_model.state_dict(), f'{model_dir}/best_text_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 765/2480 [14:33<32:54,  1.15s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX81gXKykpf9"
      },
      "source": [
        "projector = [2048, CONFIG['embedding_size']]\n",
        "best_image_model = ImageFeatureExtractor(projector=projector, pool=CONFIG['pool']).cuda()\n",
        "projector=[768, CONFIG['embedding_size']]\n",
        "best_text_model = TextExtractorModel(CONFIG['text_model_name'], projector=projector).cuda()\n",
        "\n",
        "best_image_model.load_state_dict(torch.load(f'{model_dir}/best_image_model.pt'))\n",
        "best_text_model.load_state_dict(torch.load(f'{model_dir}/best_text_model.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq3VeGWd24-p"
      },
      "source": [
        "# Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDnIDpmgwSjo"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import top_k_accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VmXsJY1b0yy"
      },
      "source": [
        "from torch.nn.functional import normalize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDOsuxoVmt6k"
      },
      "source": [
        "test_captions = df_test['caption_title_and_reference_description']\n",
        "text_model.eval()\n",
        "test_caption_tokens = []\n",
        "for caption in tqdm(test_captions):\n",
        "  inputs = CONFIG[\"tokenizer\"].encode_plus(\n",
        "              caption,\n",
        "              truncation=True,\n",
        "              add_special_tokens=True,\n",
        "              max_length=CONFIG[\"max_length\"],\n",
        "              padding='max_length'\n",
        "  )\n",
        "  ids = torch.tensor(inputs['input_ids'], dtype=torch.long).cuda().unsqueeze(0)\n",
        "  mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).cuda().unsqueeze(0)\n",
        "  text_features = normalize(best_text_model(ids, mask))\n",
        "  text_features = text_features.cpu().detach().numpy()\n",
        "  test_caption_tokens.append(text_features.squeeze())\n",
        "test_caption_base = np.stack(test_caption_tokens, axis=1).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w3IN7twoZ2O"
      },
      "source": [
        "caption_retriever = NearestNeighbors(n_neighbors=5, p=2).fit(test_caption_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh-Z25rHofJZ"
      },
      "source": [
        "relevant_k = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUCg_JllnlOj"
      },
      "source": [
        "best_image_model.eval()\n",
        "preds = []\n",
        "for index, item in enumerate(tqdm(test_loader)):\n",
        "  img = item['image'].cuda()\n",
        "  mask = item['mask'].cuda()\n",
        "  ids = item['ids'].cuda()          \n",
        "  \n",
        "  image_outputs = best_image_model(img).cpu().detach().numpy()\n",
        "  _, retrieved_ids = caption_retriever.kneighbors(image_outputs, n_neighbors=relevant_k)\n",
        "  preds.extend(retrieved_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDJhjf23xvsZ"
      },
      "source": [
        "targets = list(range(len(preds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rzRl0nxtMS7"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkqkhpo6owRO"
      },
      "source": [
        "### Top1 Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn9l3EUgVcfU"
      },
      "source": [
        "accuracy_score(preds, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52pTmS5go7lp"
      },
      "source": [
        "### Topk Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ_dpwefo_E4"
      },
      "source": [
        "desired_k = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcg3ZirHVkqc"
      },
      "source": [
        "count = 0\n",
        "for i in range(len(preds)):\n",
        "  true = targets[i]\n",
        "  if true in preds[i][:desired_k]:\n",
        "    count += 1\n",
        "\n",
        "print(count / len(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTvXolOKg6TW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}